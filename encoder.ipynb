{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "plt.ion()  \n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    # This is a utility function you can use for all your ML applications\n",
    "    # More generic than you would need for this particular assignment\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(57)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ankan/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/ankan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ankan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "baseModel = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
    "print(baseModel.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = 'ex.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(image).unsqueeze(0)  # Add a batch dimension\n",
    "input_tensor = input_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0, Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: features.1, Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: features.2, Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: features.3, Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: features.4, Output Shape: torch.Size([1, 64, 112, 112])\n",
      "Layer: features.5, Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: features.6, Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: features.7, Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: features.8, Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: features.9, Output Shape: torch.Size([1, 128, 56, 56])\n",
      "Layer: features.10, Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: features.11, Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: features.12, Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: features.13, Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: features.14, Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: features.15, Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: features.16, Output Shape: torch.Size([1, 256, 28, 28])\n",
      "Layer: features.17, Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: features.18, Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: features.19, Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: features.20, Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: features.21, Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: features.22, Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: features.23, Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: features.24, Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: features.25, Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: features.26, Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: features.27, Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: features.28, Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: features.29, Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: features.30, Output Shape: torch.Size([1, 512, 7, 7])\n",
      "Layer: features, Output Shape: torch.Size([1, 512, 7, 7])\n",
      "Layer: avgpool, Output Shape: torch.Size([1, 512, 7, 7])\n",
      "Layer: classifier.0, Output Shape: torch.Size([1, 4096])\n",
      "Layer: classifier.1, Output Shape: torch.Size([1, 4096])\n",
      "Layer: classifier.2, Output Shape: torch.Size([1, 4096])\n",
      "Layer: classifier.3, Output Shape: torch.Size([1, 4096])\n",
      "Layer: classifier.4, Output Shape: torch.Size([1, 4096])\n",
      "Layer: classifier.5, Output Shape: torch.Size([1, 4096])\n",
      "Layer: classifier.6, Output Shape: torch.Size([1, 1000])\n",
      "Layer: classifier, Output Shape: torch.Size([1, 1000])\n",
      "Layer: , Output Shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "intermediate_outputs = {}\n",
    "\n",
    "def hook_fn(module, input, output, layer_name):\n",
    "    intermediate_outputs[layer_name] = output\n",
    "\n",
    "# Register hooks on the desired layers to record intermediate outputs\n",
    "for layer_name, layer in baseModel.named_modules():\n",
    "    layer.register_forward_hook(lambda module, input, output, layer_name=layer_name: hook_fn(module, input, output, layer_name))\n",
    "\n",
    "# Forward pass the image through the model\n",
    "baseModel.eval()\n",
    "output = baseModel(input_tensor)\n",
    "\n",
    "# Access intermediate layer outputs\n",
    "for layer_name, output_tensor in intermediate_outputs.items():\n",
    "    print(f\"Layer: {layer_name}, Output Shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
